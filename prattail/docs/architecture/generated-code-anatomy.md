# Generated Code Anatomy

**Annotated walkthrough of the code PraTTaIL generates for a simple two-category language.**

---

## Example Language: TypedCalc

This document uses a two-category calculator with `Int` (native i32) and `Bool` (native
bool) to illustrate every piece of generated code. The language definition:

```rust
language! {
    name: TypedCalc,
    types {
        ![i32] as Int
        ![bool] as Bool
    },
    terms {
        // Int rules
        Add . a:Int, b:Int |- a "+" b : Int;
        Mul . a:Int, b:Int |- a "*" b : Int;

        // Bool rules
        Not . a:Bool |- "not" a : Bool;

        // Cross-category: Int comparison producing Bool
        Eq . a:Int, b:Int |- a "==" b : Bool;
    },
}
```

This language has:
- Two categories (`Int`, `Bool`), with `Int` as primary
- Two same-category infix operators (`+`, `*`) on `Int`
- One prefix operator (`not`) on `Bool`
- One cross-category rule (`==`: `Int` x `Int` -> `Bool`)
- Native types triggering integer literals and boolean literals

---

## Part 1: Token Enum (Zero-Copy)

**Source module:** `automata/codegen.rs` (`write_token_enum`)

The Token enum uses a lifetime parameter for zero-copy lexing. String-carrying variants
borrow from the input string rather than allocating new Strings during lexing.

```rust
// ======================================================================
// Generated by PraTTaIL -- Pratt + RD parser generator for MeTTaIL
// ======================================================================

#[derive(Debug, Clone, PartialEq)]
pub enum Token<'a> {
    /// End of input
    Eof,                     // <-- Always present

    /// Identifier (borrows from input)
    Ident(&'a str),          // <-- Zero-copy: &'a str instead of String

    /// Integer literal
    Integer(i64),            // <-- Present because Int has native type i32

    /// Boolean literal
    Boolean(bool),           // <-- Present because Bool has native type bool

    /// Terminal: `+`
    Plus,                    // <-- From Add rule: a "+" b

    /// Terminal: `*`
    Star,                    // <-- From Mul rule: a "*" b

    /// Terminal: `==`
    EqEq,                    // <-- From Eq rule: a "==" b

    /// Terminal: `not`
    KwNot,                   // <-- From Not rule: "not" a

    /// Terminal: `true`
    KwTrue,                  // <-- Auto-injected for bool native type

    /// Terminal: `false`
    KwFalse,                 // <-- Auto-injected for bool native type

    /// Terminal: `(`
    LParen,                  // <-- From grouping (always present for parens)

    /// Terminal: `)`
    RParen,                  // <-- From grouping (always present for parens)
}
```

**Zero-copy design:** The lexer's `lex<'a>(input: &'a str)` function produces
`Token<'a>` values that borrow directly from the input string. `Ident(&'a str)` avoids
a `String` allocation per identifier token. String allocation is deferred to AST
construction, where `(*name).to_string()` is called only at `get_or_create_var` call
sites.

**Naming convention** (`terminal_to_variant_name` in `automata/codegen.rs`):

| Terminal | Variant | Rule |
|---|---|---|
| `+` | `Plus` | Single-char operator lookup table |
| `*` | `Star` | Single-char operator lookup table |
| `==` | `EqEq` | Multi-char operator lookup table |
| `not` | `KwNot` | Alphanumeric -> `Kw` + capitalized |
| `error` | `KwError` | Alphanumeric -> `Kw` + capitalized |
| `(` | `LParen` | Delimiter lookup table |
| `{}` | `EmptyBraces` | Special multi-char lookup table |

---

## Part 2: Structured Source Locations

**Source module:** `automata/codegen.rs` (`write_position_and_range_defs`)

```rust
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub struct Span {
    pub start: usize,        // <-- Byte offset of token start in input
    pub end: usize,          // <-- Byte offset of token end (exclusive)
}

/// A position in a source file (line, column, byte offset).
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub struct Position {
    pub line: usize,         // <-- 1-based line number
    pub column: usize,       // <-- 1-based column number
    pub byte_offset: usize,  // <-- 0-based byte offset in input
}

/// A range of source text (start and end positions).
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub struct Range {
    pub start: Position,
    pub end: Position,
}
```

---

## Part 3: Parse Error Types

**Source module:** `automata/codegen.rs` (`write_parse_error_enum`)

```rust
/// Structured parse error with source location information.
#[derive(Debug, Clone, PartialEq)]
pub enum ParseError {
    /// Found a token that doesn't fit the grammar at this position.
    UnexpectedToken {
        expected: String,         // <-- e.g., "Int expression"
        found: String,            // <-- e.g., "'+'"
        span: Span,
        file_id: usize,
    },
    /// Reached end of input when more tokens were expected.
    UnexpectedEof {
        expected: String,
        file_id: usize,
    },
    /// Successfully parsed an expression but tokens remain.
    TrailingTokens {
        found: String,
        span: Span,
        file_id: usize,
    },
    /// A literal value could not be parsed (e.g., integer overflow).
    InvalidLiteral {
        message: String,
        span: Span,
        file_id: usize,
    },
}
```

---

## Part 4: Equivalence Class Table

**Source module:** `automata/codegen.rs` (`write_class_table`)
**Computed by:** `automata/partition.rs` (`compute_equivalence_classes`)

```rust
static CHAR_CLASS: [u8; 256] = [
    //  NUL SOH STX ETX EOT ENQ ACK BEL  BS  HT  LF  VT  FF  CR  SO  SI
        0,  0,  0,  0,  0,  0,  0,  0,   0,  0,  0,  0,  0,  0,  0,  0,
    // ... (256 entries mapping each byte to its equivalence class)
];

const NUM_CLASSES: usize = 20;

fn is_whitespace(b: u8) -> bool {
    matches!(b, b' ' | b'\t' | b'\n' | b'\r')
}
```

Typically 12-25 equivalence classes for realistic grammars (~15x compression from 256).

---

## Part 5: Lexer Function (Zero-Copy)

**Source module:** `automata/codegen.rs` (`write_direct_coded_lexer`)

```rust
/// Lex a complete input string into a vector of (Token, Span) pairs.
/// Zero-copy: tokens borrow from the input string.
pub fn lex<'a>(input: &'a str) -> Result<Vec<(Token<'a>, Span)>, String> {
    let bytes = input.as_bytes();
    let mut pos: usize = 0;
    let mut tokens: Vec<(Token<'a>, Span)> = Vec::new();

    while pos < bytes.len() {
        // Skip whitespace
        while pos < bytes.len() && is_whitespace(bytes[pos]) {
            pos += 1;
        }
        if pos >= bytes.len() {
            break;
        }

        let start = pos;
        let mut state: u32 = 0;
        let mut last_accept: Option<(u32, usize)> = None;

        // Maximal munch: run DFA until dead state, record last accept
        while pos < bytes.len() {
            let class = CHAR_CLASS[bytes[pos] as usize];
            let next = dfa_next(state, class);
            if next == u32::MAX { break; }
            state = next;
            pos += 1;
            if accept_token(state, &input[start..pos]).is_some() {
                last_accept = Some((state, pos));
            }
        }

        match last_accept {
            Some((accept_state, end)) => {
                pos = end;
                let text = &input[start..end];
                if let Some(token) = accept_token(accept_state, text) {
                    tokens.push((token, Span { start, end }));
                }
            }
            None => {
                return Err(format!(
                    "unexpected character '{}' at position {}",
                    bytes[start] as char, start
                ));
            }
        }
    }

    tokens.push((Token::Eof, Span { start: pos, end: pos }));
    Ok(tokens)
}
```

### Accept Token Function (Zero-Copy)

```rust
fn accept_token<'a>(state: u32, text: &'a str) -> Option<Token<'a>> {
    match state {
        1 => Some(Token::LParen),
        2 => Some(Token::RParen),
        3 => Some(Token::Star),
        4 => Some(Token::Plus),
        5 => Some(Token::Integer(
            text.parse::<i64>().expect("invalid integer literal")
        )),
        7 => Some(Token::Ident(text)),              // <-- borrows &'a str, no allocation
        11 => Some(Token::EqEq),
        13 => Some(Token::KwNot),
        14 => Some(Token::Boolean(true)),
        15 => Some(Token::Boolean(false)),
        _ => None,
    }
}
```

**Key design point:** `Token::Ident(text)` borrows a `&'a str` slice directly from the
input. No `to_string()` call during lexing. String allocation only happens later at AST
construction when `get_or_create_var((*name).to_string())` is called.

---

## Part 6: Parser Helper Functions (Two Lifetimes)

**Source module:** `pratt.rs` (`write_parser_helpers`)

```rust
fn expect_token<'a>(
    tokens: &[(Token<'a>, Span)],
    pos: &mut usize,
    predicate: impl Fn(&Token<'a>) -> bool,
    expected: &str,
) -> Result<(), String> {
    // ... (matches token, advances pos, returns error if not found)
}

fn expect_ident<'a>(
    tokens: &[(Token<'a>, Span)],
    pos: &mut usize,
) -> Result<String, String> {
    // ... (matches Token::Ident, returns name.to_string())
}

/// Two lifetimes: 'a for token content, 'b for the slice reference.
/// Using a single lifetime causes ambiguous lifetime errors with zero-copy tokens.
fn peek_token<'a, 'b>(
    tokens: &'b [(Token<'a>, Span)],
    pos: usize,
) -> Option<&'b Token<'a>> {
    tokens.get(pos).map(|(t, _)| t)
}

fn peek_ahead<'a, 'b>(
    tokens: &'b [(Token<'a>, Span)],
    pos: usize,
    offset: usize,
) -> Option<&'b Token<'a>> {
    tokens.get(pos + offset).map(|(t, _)| t)
}
```

---

## Part 7: Recovery Helper Functions

**Source module:** `pratt.rs` (`write_recovery_helpers`)

These functions are generated once and shared by all category-specific recovering parsers.

```rust
/// Advance past error tokens to the next synchronization point.
fn sync_to<'a>(
    tokens: &[(Token<'a>, Span)],
    pos: &mut usize,
    is_sync: impl Fn(&Token<'a>) -> bool,
) {
    while *pos < tokens.len() && !is_sync(&tokens[*pos].0) {
        *pos += 1;
    }
}

/// Token insertion repair: if the expected token is present, consume it.
/// If not, record an error but continue (pretend the token was there).
fn expect_token_rec<'a>(
    tokens: &[(Token<'a>, Span)],
    pos: &mut usize,
    predicate: impl Fn(&Token<'a>) -> bool,
    expected: &str,
    errors: &mut Vec<ParseError>,
) -> bool {
    if *pos < tokens.len() && predicate(&tokens[*pos].0) {
        *pos += 1;
        true
    } else {
        errors.push(ParseError::UnexpectedToken {
            expected: expected.to_string(),
            found: format!("{:?}", tokens.get(*pos).map(|(t, _)| t)),
            span: tokens.get(*pos).map(|(_, s)| *s).unwrap_or(Span { start: 0, end: 0 }),
            file_id: 0,
        });
        false // Don't advance -- token insertion repair
    }
}

/// Identifier recovery: returns "__error__" placeholder on failure.
fn expect_ident_rec<'a>(
    tokens: &[(Token<'a>, Span)],
    pos: &mut usize,
    errors: &mut Vec<ParseError>,
) -> String {
    if let Some(Token::Ident(name)) = tokens.get(*pos).map(|(t, _)| t) {
        let s = (*name).to_string();
        *pos += 1;
        s
    } else {
        errors.push(ParseError::UnexpectedToken {
            expected: "identifier".to_string(),
            found: format!("{:?}", tokens.get(*pos).map(|(t, _)| t)),
            span: tokens.get(*pos).map(|(_, s)| *s).unwrap_or(Span { start: 0, end: 0 }),
            file_id: 0,
        });
        "__error__".to_string()
    }
}
```

---

## Part 8: Sync Predicates (Per Category)

**Source module:** `prediction.rs` (`generate_sync_predicate`)

```rust
/// Sync predicate for Int: identifies tokens where the parser can
/// resume after an error. Derived from FOLLOW(Int) + structural delimiters.
fn is_sync_Int<'a>(token: &Token<'a>) -> bool {
    matches!(token,
        Token::Eof
        | Token::RParen
        | Token::Plus      // <-- In FOLLOW(Int) from infix rules
        | Token::Star      // <-- In FOLLOW(Int) from infix rules
        | Token::EqEq      // <-- In FOLLOW(Int) from cross-category Eq rule
    )
}
```

Only structural delimiters that actually appear in the grammar's terminal set are
included. For example, `Token::Semi` would not appear unless `;` is a terminal in
the grammar.

---

## Part 9: Binding Power Functions

**Source module:** `pratt.rs` (`write_bp_function`, `write_make_infix`)

```rust
fn infix_bp_Int<'a>(token: &Token<'a>) -> Option<(u8, u8)> {
    match token {
        Token::Plus => Some((2, 3)),    // <-- Add rule, left-assoc
        Token::Star => Some((4, 5)),    // <-- Mul rule, left-assoc
        _ => None,
    }
}

fn make_infix_Int<'a>(token: &Token<'a>, lhs: Int, rhs: Int) -> Int {
    match token {
        Token::Plus => Int::Add(Box::new(lhs), Box::new(rhs)),
        Token::Star => Int::Mul(Box::new(lhs), Box::new(rhs)),
        _ => unreachable!("make_infix called with non-infix token"),
    }
}
```

### Postfix Operators (when present)

For grammars with postfix operators (e.g., `Fact . a:Int |- a "!" : Int`):

```rust
fn postfix_bp_Int<'a>(token: &Token<'a>) -> Option<u8> {
    match token {
        Token::Bang => Some(14),   // <-- higher than all infix/prefix BPs
        _ => None,
    }
}

fn make_postfix_Int<'a>(token: &Token<'a>, operand: Int) -> Int {
    match token {
        Token::Bang => Int::Fact(Box::new(operand)),
        _ => unreachable!("make_postfix called with non-postfix token"),
    }
}
```

### Mixfix Operators (when present)

For grammars with N-ary operators (e.g., `Tern . c:Int, t:Int, e:Int |- c "?" t ":" e : Int`):

```rust
fn mixfix_bp_Int<'a>(token: &Token<'a>) -> Option<u8> {
    match token {
        Token::Question => Some(2),  // <-- lowest precedence (assigned first)
        _ => None,
    }
}

fn handle_mixfix_Int<'a>(
    token: &Token<'a>,
    lhs: Int,
    tokens: &[(Token<'a>, Span)],
    pos: &mut usize,
) -> Result<Int, String> {
    match token {
        Token::Question => {
            let t = parse_Int(tokens, pos, 0)?;          // parse "then" operand
            expect_token(tokens, pos,
                |t| matches!(t, Token::Colon), ":")?;     // expect ":"
            let e = parse_Int(tokens, pos, 3)?;           // parse "else" operand
            Ok(Int::Tern(Box::new(lhs), Box::new(t), Box::new(e)))
        },
        _ => unreachable!(),
    }
}
```

---

## Part 10: Pratt Parser (Int Category)

**Source module:** `pratt.rs` (`write_pratt_parser`)

```rust
fn parse_Int<'a>(
    tokens: &[(Token<'a>, Span)],
    pos: &mut usize,
    min_bp: u8,
) -> Result<Int, String> {
    let mut lhs = parse_Int_prefix(tokens, pos)?;

    loop {
        if *pos >= tokens.len() { break; }
        let token = &tokens[*pos].0;

        // Check postfix operators first (highest binding power)
        if let Some(bp) = postfix_bp_Int(token) {
            if bp < min_bp { break; }
            let op = token.clone();
            *pos += 1;
            lhs = make_postfix_Int(&op, lhs);
            continue;
        }

        // Check mixfix operators (lowest precedence, before regular infix)
        if let Some(l_bp) = mixfix_bp_Int(token) {
            if l_bp < min_bp { break; }
            let op = token.clone();
            *pos += 1;
            lhs = handle_mixfix_Int(&op, lhs, tokens, pos)?;
            continue;
        }

        // Check regular infix operators
        if let Some((l_bp, r_bp)) = infix_bp_Int(token) {
            if l_bp < min_bp { break; }
            let op_token = token.clone();
            *pos += 1;
            let rhs = parse_Int(tokens, pos, r_bp)?;
            lhs = make_infix_Int(&op_token, lhs, rhs);
        } else {
            break;
        }
    }
    Ok(lhs)
}
```

**Note:** The Pratt loop checks postfix first, then mixfix, then infix. This ensures
the binding power hierarchy: `-5!` = `-(5!)` because postfix BP (14) > prefix BP (12) >
infix BP (2-9).

---

## Part 11: Prefix Handler (Int Category)

**Source module:** `pratt.rs` (`write_prefix_handler`)

```rust
fn parse_Int_prefix<'a>(
    tokens: &[(Token<'a>, Span)],
    pos: &mut usize,
) -> Result<Int, String> {
    if *pos >= tokens.len() {
        return Err(format!("unexpected end of input, expected Int expression"));
    }
    match &tokens[*pos].0 {
        // Integer literal (from native type i32)
        Token::Integer(value) => {
            let v = *value;
            *pos += 1;
            Ok(Int::NumLit(v as i32))
        },

        // Unary prefix operator (e.g., negation "-")
        // Uses prefix_bp = max_infix_bp + 2 for tight binding
        Token::Minus => {
            *pos += 1;
            let a = parse_Int(tokens, pos, 12)?;  // prefix_bp = 12
            Ok(Int::Neg(Box::new(a)))
        },

        // Grouping: parenthesized expression
        Token::LParen => {
            *pos += 1;
            let expr = parse_Int(tokens, pos, 0)?;
            expect_token(tokens, pos, |t| matches!(t, Token::RParen), ")")?;
            Ok(expr)
        },

        // Variable fallback: bare identifier
        Token::Ident(name) => {
            let var_name = (*name).to_string();   // <-- String allocation deferred to here
            *pos += 1;
            Ok(Int::IVar(
                mettail_runtime::OrdVar(mettail_runtime::Var::Free(
                    mettail_runtime::get_or_create_var(var_name)
                ))
            ))
        },

        other => Err(format!(
            "expected Int expression at position {}, found {:?}",
            *pos, other
        )),
    }
}
```

---

## Part 12: Recovering Parser (Per Category)

**Source module:** `pratt.rs` (`write_pratt_parser_recovering`)

```rust
/// Recovery-aware parser for Int. Returns Option<Int> + accumulated errors.
/// Generated alongside the fail-fast parser; zero overhead when not used.
fn parse_Int_recovering<'a>(
    tokens: &[(Token<'a>, Span)],
    pos: &mut usize,
    min_bp: u8,
    errors: &mut Vec<ParseError>,
) -> Option<Int> {
    // Try prefix; on failure, record error and sync
    let mut lhs = match parse_Int_prefix(tokens, pos) {
        Ok(v) => v,
        Err(msg) => {
            errors.push(ParseError::UnexpectedToken {
                expected: "Int expression".to_string(),
                found: msg,
                span: tokens.get(*pos).map(|(_, s)| *s)
                    .unwrap_or(Span { start: 0, end: 0 }),
                file_id: 0,
            });
            sync_to(tokens, pos, is_sync_Int);
            return None;
        }
    };

    // Infix loop with recovery
    loop {
        if *pos >= tokens.len() { break; }
        let token = &tokens[*pos].0;

        if let Some((l_bp, r_bp)) = infix_bp_Int(token) {
            if l_bp < min_bp { break; }
            let op_token = token.clone();
            *pos += 1;
            match parse_Int_recovering(tokens, pos, r_bp, errors) {
                Some(rhs) => lhs = make_infix_Int(&op_token, lhs, rhs),
                None => break,   // <-- Continue with partial AST
            }
        } else {
            break;
        }
    }
    Some(lhs)
}
```

---

## Part 13: Cross-Category Dispatch (Bool)

**Source module:** `dispatch.rs` (`write_category_dispatch`)

```rust
fn parse_Bool<'a>(
    tokens: &[(Token<'a>, Span)],
    pos: &mut usize,
    min_bp: u8,
) -> Result<Bool, String> {
    if *pos >= tokens.len() {
        return Err(format!("unexpected end of input, expected Bool expression"));
    }
    match &tokens[*pos].0 {
        // Integer is UNIQUE to Int -- unambiguous cross-category
        Token::Integer(_) => {
            let left = parse_Int(tokens, pos, 0)?;
            expect_token(tokens, pos, |t| matches!(t, Token::EqEq), "==")?;
            let right = parse_Int(tokens, pos, 0)?;
            Ok(Bool::Eq(Box::new(left), Box::new(right)))
        },

        // Ident is AMBIGUOUS -- save/restore backtracking
        Token::Ident(_) => {
            let saved = *pos;
            if let Ok(left) = parse_Int(tokens, pos, 0) {
                if peek_token(tokens, *pos).map_or(false, |t| matches!(t, Token::EqEq)) {
                    *pos += 1;
                    let right = parse_Int(tokens, pos, 0)?;
                    return Ok(Bool::Eq(Box::new(left), Box::new(right)));
                }
            }
            *pos = saved;
            parse_Bool_own(tokens, pos, min_bp)
        },

        _ => parse_Bool_own(tokens, pos, min_bp),
    }
}
```

---

## Part 14: Parse Entry Points (4 per Category)

**Source module:** `pratt.rs` (`write_parse_entry_points`)

```rust
impl Int {
    /// Parse a string as an Int expression (fail-fast).
    pub fn parse(input: &str) -> Result<Int, String> {
        let tokens = lex(input)?;
        let mut pos = 0;
        let result = parse_Int(&tokens, &mut pos, 0)?;
        if pos < tokens.len() && !matches!(tokens[pos].0, Token::Eof) {
            return Err(format!(
                "unexpected token {:?} at position {} after parsing",
                tokens[pos].0, pos
            ));
        }
        Ok(result)
    }

    /// Parse with file ID tracking for multi-file error reporting.
    pub fn parse_with_file_id(input: &str, file_id: usize) -> Result<Int, String> {
        let tokens = lex(input)?;
        let mut pos = 0;
        let result = parse_Int(&tokens, &mut pos, 0)?;
        if pos < tokens.len() && !matches!(tokens[pos].0, Token::Eof) {
            return Err(format!(
                "unexpected token {:?} at position {} in file {} after parsing",
                tokens[pos].0, pos, file_id
            ));
        }
        Ok(result)
    }

    /// Parse with error recovery -- collects multiple errors instead of
    /// stopping at the first one. Returns partial AST + all errors.
    pub fn parse_recovering(input: &str) -> (Option<Int>, Vec<ParseError>) {
        let tokens = match lex(input) {
            Ok(t) => t,
            Err(e) => return (None, vec![ParseError::InvalidLiteral {
                message: e, span: Span { start: 0, end: 0 }, file_id: 0,
            }]),
        };
        let mut pos = 0;
        let mut errors = Vec::new();
        let result = parse_Int_recovering(&tokens, &mut pos, 0, &mut errors);
        if pos < tokens.len() && !matches!(tokens[pos].0, Token::Eof) {
            errors.push(ParseError::TrailingTokens {
                found: format!("{:?}", tokens[pos].0),
                span: tokens[pos].1,
                file_id: 0,
            });
        }
        (result, errors)
    }

    /// Parse with recovery + file ID tracking.
    pub fn parse_recovering_with_file_id(
        input: &str, file_id: usize,
    ) -> (Option<Int>, Vec<ParseError>) {
        // Same as parse_recovering but sets file_id on all ParseError instances
        // ...
    }
}

impl Bool {
    pub fn parse(input: &str) -> Result<Bool, String> { /* ... */ }
    pub fn parse_with_file_id(input: &str, file_id: usize) -> Result<Bool, String> { /* ... */ }
    pub fn parse_recovering(input: &str) -> (Option<Bool>, Vec<ParseError>) { /* ... */ }
    pub fn parse_recovering_with_file_id(
        input: &str, file_id: usize,
    ) -> (Option<Bool>, Vec<ParseError>) { /* ... */ }
}
```

---

## Generated Code Size Estimate

| Component | Approximate Lines |
|---|---|
| Token<'a> enum | ~25 |
| Span, Position, Range structs | ~15 |
| ParseError enum | ~25 |
| CHAR_CLASS table | ~20 |
| Whitespace helper | ~3 |
| lex<'a>() function | ~45 |
| dfa_next() | ~40 |
| accept_token<'a>() | ~15 |
| Parser helpers (expect, peek) | ~35 |
| Recovery helpers (sync, rec) | ~40 |
| infix_bp() (Int) | ~8 |
| make_infix() (Int) | ~8 |
| postfix_bp() (if present) | ~8 |
| make_postfix() (if present) | ~8 |
| mixfix_bp() (if present) | ~8 |
| handle_mixfix() (if present) | ~15 |
| parse_Int() | ~30 |
| parse_Int_prefix() | ~30 |
| parse_Int_recovering() | ~35 |
| is_sync_Int() | ~8 |
| parse_Bool() with dispatch | ~35 |
| parse_Bool_own() | ~10 |
| parse_Bool_prefix() | ~30 |
| parse_Bool_recovering() | ~35 |
| is_sync_Bool() | ~8 |
| parse_not() | ~8 |
| Parse entry points (4 x 2 categories) | ~80 |
| **Total (TypedCalc)** | **~700** |

The generated code is built entirely as a `String` buffer using `write!` formatting,
then parsed once via `str::parse::<TokenStream>()`. This string-based codegen approach
produces ~20% fewer total lines across the 4 MeTTaIL languages compared to the original
`quote!`-based approach, while eliminating thousands of intermediate `TokenStream`
allocations during generation.

---

## Part 15: Lambda and Application Codegen

Lambda and application handlers are generated for the **primary category** only.
Using RhoCalc (primary category: `Proc`) as an example:

### Single Lambda: `^x.{body}`

```rust
// Generated prefix dispatch arm
Token::Caret => { parse_lambda(tokens, pos) }

// Generated parse_lambda function (simplified)
fn parse_lambda<'a>(tokens: &[(Token<'a>, Span)], pos: &mut usize) -> Result<Proc, ParseError> {
    expect_token(tokens, pos, |t| matches!(t, Token::Caret), "^")?;
    match peek_token(tokens, *pos) {
        Some(Token::Ident(_)) => {
            let binder_name = expect_ident(tokens, pos)?;
            expect_token(tokens, pos, |t| matches!(t, Token::Dot), ".")?;
            expect_token(tokens, pos, |t| matches!(t, Token::LBrace), "{")?;
            let body = parse_Proc(tokens, pos, 0)?;
            expect_token(tokens, pos, |t| matches!(t, Token::RBrace), "}")?;
            // Inference-driven variant selection
            let inferred = body.infer_var_type(&binder_name);
            let scope = mettail_runtime::Scope::new(
                mettail_runtime::Binder(mettail_runtime::get_or_create_var(binder_name)),
                Box::new(body),
            );
            Ok(match inferred {
                Some(InferredType::Base(VarCategory::Proc)) => Proc::LamProc(scope),
                Some(InferredType::Base(VarCategory::Name)) => Proc::LamName(scope),
                Some(InferredType::Base(VarCategory::Int))  => Proc::LamInt(scope),
                Some(InferredType::Base(VarCategory::Bool)) => Proc::LamBool(scope),
                _ => Proc::LamProc(scope)  // default to primary category
            })
        }
        Some(Token::LBracket) => { /* multi-binder: ^[x,y].{body} → MLam variant */ }
        _ => Err(ParseError::UnexpectedToken { /* ... */ })
    }
}
```

### Application: `$int(f, x)` and `$$int(f, x, y)`

```rust
// Single application: $int(f, x) → Proc::ApplyInt(f, x)
fn parse_dollar_int<'a>(tokens: &[(Token<'a>, Span)], pos: &mut usize) -> Result<Proc, ParseError> {
    expect_token(tokens, pos, |t| matches!(t, Token::DollarInt), "$int")?;
    expect_token(tokens, pos, |t| matches!(t, Token::LParen), "(")?;
    let f = parse_Proc(tokens, pos, 0)?;    // lambda is a Proc
    expect_token(tokens, pos, |t| matches!(t, Token::Comma), ",")?;
    let x = parse_Int(tokens, pos, 0)?;     // argument parsed as Int (the domain)
    expect_token(tokens, pos, |t| matches!(t, Token::RParen), ")")?;
    Ok(Proc::ApplyInt(Box::new(f), Box::new(x)))
}

// Multi-application: $$int(f, x1, x2) → Proc::MApplyInt(f, [x1, x2])
fn parse_ddollar_int<'a>(tokens: &[(Token<'a>, Span)], pos: &mut usize) -> Result<Proc, ParseError> {
    expect_token(tokens, pos, |t| matches!(t, Token::DdollarIntLp), "$$int(")?;
    let f = parse_Proc(tokens, pos, 0)?;
    expect_token(tokens, pos, |t| matches!(t, Token::Comma), ",")?;
    let mut args: Vec<Int> = Vec::new();
    loop {
        let arg = parse_Int(tokens, pos, 0)?;
        args.push(arg);
        if peek_token(tokens, *pos).map_or(false, |t| matches!(t, Token::Comma)) {
            *pos += 1;
        } else {
            break;
        }
    }
    expect_token(tokens, pos, |t| matches!(t, Token::RParen), ")")?;
    Ok(Proc::MApplyInt(Box::new(f), args))
}
```

Note that `$$int(` is a **single token** (`Token::DdollarIntLp`) — the opening
parenthesis is consumed as part of the token to avoid ambiguity with `$int`.

---

## Part 16: Binder and Scope Codegen

Binder rules (e.g., PInputs in RhoCalc) use `moniker`'s `Scope`, `Binder`,
and `FreeVar` types for proper name binding:

### Single-Binder Construction

```rust
// For ^x.{body}:
let scope = mettail_runtime::Scope::new(
    mettail_runtime::Binder(mettail_runtime::get_or_create_var(binder_name)),
    //                       ↑ Binder wraps FreeVar<String>
    Box::new(body),
);
```

### Multi-Binder Construction

```rust
// For ^[x,y,z].{body}:
let binders: Vec<mettail_runtime::Binder<String>> =
    binder_names.into_iter()
        .map(|s| mettail_runtime::Binder(mettail_runtime::get_or_create_var(s)))
        .collect();
let scope = mettail_runtime::Scope::new(binders, Box::new(body));
// Result: Proc::MLamInt(scope) (variant selected by infer_var_type)
```

### Structural Binder Rules (PInputs)

The RhoCalc `PInputs` rule `(n?x, m?y).{body}` generates a ZipMapSep
handler that constructs binders from the `?`-separated pairs:

```rust
// Simplified generated code for PInputs
let binder = mettail_runtime::Binder(mettail_runtime::get_or_create_var(x_name));
let scope = mettail_runtime::Scope::new(binders_vec, Box::new(body));
Proc::PInputs(names_vec, scope)
```

All `NonTerminal` fields in the AST are `Box<T>` — constructors always wrap
sub-expressions with `Box::new()`.

---

## Part 17: Collection Codegen

Collection rules (HashBag, HashSet, Vec) generate a parsing loop with
separator checking:

### HashBag Example: PPar (`{P | P | ...}`)

```rust
// Generated code for PPar (simplified from recursive.rs)
fn parse_ppar<'a>(tokens: &[(Token<'a>, Span)], pos: &mut usize) -> Result<Proc, ParseError> {
    expect_token(tokens, pos, |t| matches!(t, Token::LBrace), "{")?;
    let mut ps = mettail_runtime::HashBag::new();
    loop {
        match parse_Proc(tokens, pos, 0) {
            Ok(elem) => {
                ps.insert(elem);
                // Check for separator "|"
                if peek_token(tokens, *pos).map_or(false, |t| matches!(t, Token::Pipe)) {
                    *pos += 1;  // consume separator, continue loop
                } else {
                    break;      // no separator → end of collection
                }
            }
            Err(_) => break,    // parse failure → end of collection (may be empty)
        }
    }
    expect_token(tokens, pos, |t| matches!(t, Token::RBrace), "}")?;
    Ok(Proc::PPar(ps))
}
```

### Collection Kind Dispatch

The collection init and insert method are selected by `CollectionKind`:

| Kind | Init Expression | Insert Method |
|---|---|---|
| `HashBag` | `mettail_runtime::HashBag::new()` | `.insert(elem)` |
| `HashSet` | `std::collections::HashSet::new()` | `.insert(elem)` |
| `Vec` | `Vec::new()` | `.push(elem)` |

The loop structure is identical for all collection kinds — only the
initialization and insertion expressions change.

---

## Tracing an Example Parse

**Input:** `"3 + x * 2 == 5"`

### Lexing (Zero-Copy)

```
lex("3 + x * 2 == 5")
  -> [(Integer(3), 0..1),
      (Plus, 2..3),
      (Ident("x"), 4..5),        // &str borrowed from input, no String alloc
      (Star, 6..7),
      (Integer(2), 8..9),
      (EqEq, 10..12),
      (Integer(5), 13..14),
      (Eof, 14..14)]
```

### Parsing as Bool (top-level entry)

```
Bool::parse("3 + x * 2 == 5")
  1. lex -> tokens (zero-copy)
  2. parse_Bool(tokens, pos=0, min_bp=0)
  3. See Integer(3) -> unique to Int -> cross-category path
  4. parse_Int(tokens, pos=0, min_bp=0)
     a. prefix: Integer(3) -> NumLit(3), pos=1
     b. infix loop: Plus, l_bp=2 >= 0, consume +, pos=2
     c. parse_Int(tokens, pos=2, min_bp=3)
        i.  prefix: Ident("x") -> IVar(x), pos=3   // "x".to_string() here
        ii. infix: Star, l_bp=4 >= 3, consume *, pos=4
        iii. parse_Int(tokens, pos=4, min_bp=5)
             prefix: Integer(2) -> NumLit(2), pos=5
             infix: EqEq -> not an Int infix op -> break
             return NumLit(2)
        iv. make_infix(Star, IVar(x), NumLit(2)) -> Mul(x, 2)
        v.  infix: EqEq -> not an Int infix op -> break
        vi. return Mul(x, 2)
     d. make_infix(Plus, NumLit(3), Mul(x, 2)) -> Add(3, Mul(x, 2))
     e. infix: EqEq -> not an Int infix op -> break
     f. return Add(3, Mul(x, 2))
  5. left = Add(3, Mul(x, 2))
  6. See EqEq -> expect "==" -> pos=6
  7. parse_Int(tokens, pos=6, min_bp=0)
     prefix: Integer(5) -> NumLit(5), pos=7
     no infix -> return NumLit(5)
  8. right = NumLit(5)
  9. return Bool::Eq(Box::new(Add(3, Mul(x, 2))), Box::new(NumLit(5)))
```

**Result:** `Eq(Add(NumLit(3), Mul(IVar(x), NumLit(2))), NumLit(5))`

### Error Recovery Example

**Input:** `"3 + + 5"` (missing operand)

```
Int::parse_recovering("3 + + 5")
  1. lex -> tokens: [Integer(3), Plus, Plus, Integer(5), Eof]
  2. parse_Int_recovering(tokens, pos=0, min_bp=0, errors=[])
  3. prefix: Integer(3) -> NumLit(3), pos=1
  4. infix: Plus, l_bp=2 >= 0, consume +, pos=2
  5. parse_Int_recovering(tokens, pos=2, min_bp=3, errors)
     a. prefix: Plus at pos=2 -> error! "expected Int expression"
     b. errors.push(UnexpectedToken { expected: "Int expression", found: "Plus", ... })
     c. sync_to(tokens, pos, is_sync_Int)
        Plus is in FOLLOW(Int) (it's an infix op) -> sync stops at pos=2
     d. return None
  6. rhs = None -> break infix loop
  7. return Some(NumLit(3)), errors = [UnexpectedToken(...)]

Result: (Some(NumLit(3)), [UnexpectedToken { expected: "Int expression", found: "Plus", ... }])
```
