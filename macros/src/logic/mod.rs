//! Ascent Datalog code generation
//!
//! This module orchestrates the generation of Ascent Datalog code for a theory.
//!
//! ## Structure
//!
//! - `relations` - Relation declarations (categories, equality, rewrites, projections)
//! - `categories` - Category exploration and term deconstruction rules
//! - `equations` - Equality/equation rules with congruence
//! - `rewrites/` - Base rewrite rules and pattern/RHS generation
//! - `congruence/` - Congruence rules for rewrites (collection, regular, binding)
//!
//! ## Generated Code Components
//!
//! 1. **Relations**: Declare all Ascent relations for terms, equality, and rewrites
//! 2. **Category Rules**: Explore term space via rewrites and deconstruct terms
//! 3. **Equation Rules**: Add reflexivity, congruence, and user-defined equalities
//! 4. **Rewrite Rules**: Base rewrites + congruence rules (propagate through constructors)

use crate::{ast::{grammar::GrammarItem, language::{BuiltinOp, LanguageDef, SemanticOperation}}, logic::rules::generate_base_rewrites};
use proc_macro2::{Ident, TokenStream};
use quote::{format_ident, quote};

mod categories;
mod equations;
mod relations;
mod writer;

pub mod congruence;
pub mod rules;

// Re-export key functions
pub use categories::generate_category_rules;
pub use equations::generate_equation_rules;
pub use relations::generate_relations;

// Re-export congruence function
pub use congruence::generate_all_explicit_congruences;

/// Main entry point: Generate complete Ascent source for a theory
pub fn generate_ascent_source(language: &LanguageDef) -> TokenStream {
    let theory_name = language.name.to_string().to_lowercase();
    let source_name = format_ident!("{}_source", theory_name);

    let relations = generate_relations(language);
    let category_rules = generate_category_rules(language);
    let equation_rules = generate_equation_rules(language);
    let rewrite_rules = generate_rewrite_rules(language);
    let custom_logic = language.logic.as_ref()
        .map(|l| l.content.clone())
        .unwrap_or_default();

    let result = quote! {
        ::ascent::ascent_source! {
            #source_name:

            #relations

            #category_rules

            #equation_rules

            #rewrite_rules

            #custom_logic
        }
    };

    // Format and write the generated Ascent source to file
    let formatted_source = format_ascent_source(
        &theory_name,
        &source_name,
        &relations,
        &category_rules,
        &equation_rules,
        &rewrite_rules,
        &custom_logic,
    );

    // Write to file for inspection
    if let Err(e) = writer::write_ascent_file(&language.name.to_string(), &formatted_source) {
        eprintln!("Warning: Failed to write Ascent Datalog file: {}", e);
    }

    result
}

/// Format Ascent source for display and file output
fn format_ascent_source(
    theory_name: &str,
    source_name: &Ident,
    relations: &TokenStream,
    category_rules: &TokenStream,
    equation_rules: &TokenStream,
    rewrite_rules: &TokenStream,
    custom_logic: &TokenStream,
) -> String {
    let mut output = String::new();

    output.push_str(&format!("// Generated Ascent Datalog for {} theory\n", theory_name));
    output.push_str("// This file is generated by the theory! macro and is for inspection only.\n");
    output.push_str("// Do not edit manually - changes will be overwritten.\n\n");

    output.push_str("ascent_source! {\n");
    output.push_str(&format!("    {}:\n\n", source_name));

    output.push_str("    // Relations\n");
    for line in relations.to_string().split(';') {
        output.push_str(&print_rule(line));
    }

    output.push_str("\n    // Category rules\n");
    for line in category_rules.to_string().split(';') {
        output.push_str(&print_rule(line));
    }

    output.push_str("\n    // Equation rules\n");
    for line in equation_rules.to_string().split(';') {
        output.push_str(&print_rule(line));
    }

    output.push_str("\n    // Rewrite rules\n");
    for line in rewrite_rules.to_string().split(';') {
        output.push_str(&print_rule(line));
    }

    // Custom logic (user-defined relations and rules)
    let custom_str = custom_logic.to_string();
    if !custom_str.trim().is_empty() {
        output.push_str("\n    // Custom logic\n");
        for line in custom_str.split(';') {
            output.push_str(&print_rule(line));
        }
    }

    output.push_str("}\n");

    output
}

/// Generate rewrite rules: base rewrites + explicit congruence rules
///
/// - **Base rewrites**: Rules without premises (S => T)
/// - **Explicit congruences**: Rules with premises (if S => T then LHS => RHS)
/// - **Semantic evaluation**: Rules for built-in operators (Add, Sub, etc.)
///
/// Note: Beta-reduction is NOT generated as rewrite rules. Instead, it happens
/// immediately via `normalize()` when terms are created. This makes beta-reduction
/// "invisible" - users don't see it as a separate reduction step.
///
/// Note: Rewrite congruences are NOT auto-generated. Users explicitly control
/// where rewrites propagate by writing `if S => T then ...` rules.
pub fn generate_rewrite_rules(language: &LanguageDef) -> TokenStream {
    let mut rules = Vec::new();

    // Generate base rewrite clauses (no premise)
    let base_rewrite_clauses = generate_base_rewrites(language);
    rules.extend(base_rewrite_clauses);

    // Generate semantic evaluation rules (for operators with semantics)
    let semantic_rules = generate_semantic_rules(language);
    rules.extend(semantic_rules);

    // Generate explicit congruence rules (with premise: if S => T then ...)
    // These are user-declared rules that control where rewrites propagate
    let congruence_rules = generate_all_explicit_congruences(language);
    rules.extend(congruence_rules);

    quote! {
        #(#rules)*
    }
}

/// Generate semantic evaluation rules for constructors with semantics
/// For example: Add (NumLit a) (NumLit b) => NumLit(a + b)
fn generate_semantic_rules(language: &LanguageDef) -> Vec<TokenStream> {

    let mut rules = Vec::new();

    for semantic in &language.semantics {
        let constructor_name = &semantic.constructor;

        // Extract the operator
        let op_token = match &semantic.operation {
            SemanticOperation::Builtin(builtin_op) => {
                match builtin_op {
                    BuiltinOp::Add => quote! { + },
                    BuiltinOp::Sub => quote! { - },
                    BuiltinOp::Mul => quote! { * },
                    BuiltinOp::Div => quote! { / },
                    BuiltinOp::Rem => quote! { % },
                    _ => continue, // Skip other operators
                }
            },
        };

        // Find the rule with this constructor
        if let Some(rule) = language.terms.iter().find(|r| r.label == *constructor_name) {
            // Check if this is a binary operator (should have exactly 2 non-terminal fields)
            let non_terminals: Vec<_> = rule
                .items
                .iter()
                .filter_map(|item| {
                    if let GrammarItem::NonTerminal(nt) = item {
                        Some(nt)
                    } else {
                        None
                    }
                })
                .collect();

            if non_terminals.len() == 2 {
                let category = &rule.category;
                let label = &rule.label;

                // Generate rule with proper variable extraction in the head
                let rw_rel = format_ident!("rw_{}", category.to_string().to_lowercase());
                let cat_rel = format_ident!("{}", category.to_string().to_lowercase());
                let num_lit = format_ident!("NumLit");

                // Pattern: rw_cat(s, t) with body that matches and extracts a, b
                rules.push(quote! {
                    #rw_rel(s, t) <--
                        #cat_rel(s),
                        if let #category::#label(left, right) = s,
                        if let #category::#num_lit(a) = left.as_ref(),
                        if let #category::#num_lit(b) = right.as_ref(),
                        let t = #category::#num_lit(a #op_token b);
                });
            }
        }
    }

    rules
}

pub fn split_commas_outside_parens(s: &str) -> Vec<&str> {
    let mut result = Vec::new();
    let mut depth = 0;
    let mut start = 0;

    for (i, ch) in s.char_indices() {
        match ch {
            '(' => depth += 1,
            ')' => depth -= 1,
            ',' if depth == 0 => {
                result.push(&s[start..i]);
                start = i + 1;
            },
            _ => {},
        }
    }

    // Add the last segment
    if start <= s.len() {
        result.push(&s[start..]);
    }

    result
}

/// Normalize whitespace in a string by replacing all consecutive whitespace
/// (including newlines) with a single space. This fixes formatting issues
/// from TokenStream::to_string() which can insert unwanted line breaks.
fn normalize_whitespace(s: &str) -> String {
    s.split_whitespace().collect::<Vec<_>>().join(" ")
}

pub fn print_rule(line: &str) -> String {
    if line.trim().is_empty() {
        return String::new();
    }

    // Normalize whitespace to fix TokenStream formatting issues
    let normalized = normalize_whitespace(line);

    let (head, body) = normalized
        .split_once("<- -")
        .unwrap_or((normalized.trim(), ""));
    let head_clauses = split_commas_outside_parens(head);
    let (head_last, head_rest) = head_clauses.split_last().unwrap_or((&"", &[]));
    let clauses = split_commas_outside_parens(body);
    let (last, rest) = clauses.split_last().unwrap_or((&"", &[]));
    if !body.trim().is_empty() {
        let mut result = String::new();
        for clause in head_rest {
            result.push_str(&format!("{},\n", clause.trim()));
        }
        result.push_str(&format!("{} <--\n", head_last.trim()));
        for clause in rest {
            result.push_str(&format!("    {},\n", clause.trim()));
        }
        result.push_str(&format!("    {};\n\n", last.trim()));
        result.to_string()
    } else {
        format!("{};\n\n", normalized.trim())
    }
}
